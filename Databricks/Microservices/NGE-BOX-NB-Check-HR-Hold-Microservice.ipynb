{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4644c3-38fe-42cd-87d1-4088aecbc697",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "## Import libraries\n",
    "import pyspark as ps\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "import boto3\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "#import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63ad2471-b594-4cc6-9a6f-5ebf7d281d53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the parameter values\n",
    "job_id = dbutils.widgets.get(\"job_id\")\n",
    "usecase_id = dbutils.widgets.get(\"usecase_id\")\n",
    "\n",
    "# Print the parameter values\n",
    "print(f\"job_id: {job_id}\")\n",
    "print(f\"usecase_id: {usecase_id}\")\n",
    "\n",
    "# assign the usecase variable for writing csv files\n",
    "usecase_set_1 = ['511','512','514']\n",
    "usecase_set_2 = ['513']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab8deb8b-7ca8-47d3-93f6-8aa4dbfaaf22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m\"<command-3707175531616020>\"\u001B[0;36m, line \u001B[0;32m2\u001B[0m\n",
       "\u001B[0;31m    if usecase_id = 512:\u001B[0m\n",
       "\u001B[0m                  ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m\"<command-3707175531616020>\"\u001B[0;36m, line \u001B[0;32m2\u001B[0m\n\u001B[0;31m    if usecase_id = 512:\u001B[0m\n\u001B[0m                  ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-3707175531616020>, line 2)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get s3 params from metadata table \n",
    "s3_query = f\"SELECT success_filename_prefix,error_filename_prefix,s3_bucket_name,s3_STG_path,s3_archive_path,Success_Query,Success_Header,Error_Query,Error_Header,s3_ATL_bucket_name,s3_ATL_STG_path,s3_ATL_archive_path,ATL_Error_Query,ATL_Error_Query_Header,ATL_error_filename_prefix from com_us_alyt_ngebox.metadata_usecase where USECASE_ID = '{usecase_id}'\"\n",
    "\n",
    "# assign the value to variable from the result\n",
    "s3_bucket_query_df = sqlContext.sql(s3_query).collect()[0]['s3_bucket_name']\n",
    "s3_STG_path_query_df = sqlContext.sql(s3_query).collect()[0]['s3_STG_path']\n",
    "s3_archive_path_query_df = sqlContext.sql(s3_query).collect()[0]['s3_archive_path']\n",
    "s3_success_prefix_df = sqlContext.sql(s3_query).collect()[0]['success_filename_prefix']\n",
    "s3_error_prefix_df = sqlContext.sql(s3_query).collect()[0]['error_filename_prefix']\n",
    "Success_Query = sqlContext.sql(s3_query).collect()[0]['Success_Query']\n",
    "Success_Header = sqlContext.sql(s3_query).collect()[0]['Success_Header']\n",
    "Error_Query = sqlContext.sql(s3_query).collect()[0]['Error_Query']\n",
    "Error_Header = sqlContext.sql(s3_query).collect()[0]['Error_Header']\n",
    "s3_ATL_bucket_name_df = sqlContext.sql(s3_query).collect()[0]['s3_ATL_bucket_name']\n",
    "s3_ATL_STG_path_df = sqlContext.sql(s3_query).collect()[0]['s3_ATL_STG_path']\n",
    "s3_ATL_archive_path_df = sqlContext.sql(s3_query).collect()[0]['s3_ATL_archive_path']\n",
    "ATL_Error_Query = sqlContext.sql(s3_query).collect()[0]['ATL_Error_Query']\n",
    "ATL_Error_Query_Header = sqlContext.sql(s3_query).collect()[0]['ATL_Error_Query_Header']\n",
    "ATL_error_filename_prefix_df = sqlContext.sql(s3_query).collect()[0]['ATL_error_filename_prefix']\n",
    "\n",
    "#convert header string to list by assigning index \n",
    "Success_Header_list = Success_Header.split(',')\n",
    "Error_Header_list = Error_Header.split(',')\n",
    "if usecase_id == '514':\n",
    "    ATL_Error_Query_Header_list = ATL_Error_Query_Header.split(',')\n",
    "\n",
    "# Set the S3 bucket and folder path\n",
    "s3_bucket = s3_bucket_query_df\n",
    "ATL_s3_bucket = s3_ATL_bucket_name_df\n",
    "s3_folder_path_STG = s3_STG_path_query_df\n",
    "ATL_s3_folder_path_STG = s3_ATL_STG_path_df\n",
    "s3_folder_path_arc = s3_archive_path_query_df\n",
    "ATL_s3_folder_path_arc = s3_ATL_archive_path_df\n",
    "s3_success_filename = s3_success_prefix_df\n",
    "s3_error_filename = s3_error_prefix_df\n",
    "ATL_error_filename = ATL_error_filename_prefix_df\n",
    "\n",
    "# Get the current date and time\n",
    "folder_name = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "folder_name_ATL = datetime.datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4346c28c-a842-4919-97dc-c91164b79b1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Populate success data and the headers\n",
    "s_query = f\"{Success_Query}\"\n",
    "success_query = s_query\n",
    "headers_sucess = Success_Header_list\n",
    "success_query_df = sqlContext.sql(success_query).collect()\n",
    "if len(success_query_df) > 0:\n",
    "    # Create a list to store the data\n",
    "    success_list = []\n",
    "    for curr_row in success_query_df:\n",
    "        success_list.append(curr_row.asDict(True))\n",
    "    # Create the DataFrame with data and headers\n",
    "    success_df = pd.DataFrame(success_list)\n",
    "else:\n",
    "    # Create an empty DataFrame with headers\n",
    "    success_df = pd.DataFrame(columns=headers_sucess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5675f5f7-579c-4d1f-9446-265755d08a7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Populate error data and the headers\n",
    "e_query = f\"{Error_Query}\"\n",
    "error_query = e_query\n",
    "headers_error = Error_Header_list\n",
    "error_query_df = sqlContext.sql(error_query).collect()\n",
    "if len(error_query_df) > 0:\n",
    "    # Create a list to store the data\n",
    "    error_list = []\n",
    "    for curr_row in error_query_df:\n",
    "        error_list.append(curr_row.asDict(True))\n",
    "    # Create the DataFrame with data and headers\n",
    "    error_df = pd.DataFrame(error_list)\n",
    "else:\n",
    "    # Create an empty DataFrame with headers\n",
    "    error_df = pd.DataFrame(columns=headers_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c079509-fd08-47b5-b1da-3cd975b0456b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# populate ATL data for error records\n",
    "if usecase_id == '514':\n",
    "    atl_query = f\"{ATL_Error_Query}\"\n",
    "    error_atl_query = atl_query\n",
    "    error_atl_header = ATL_Error_Query_Header_list\n",
    "    error_atl_df = sqlContext.sql(error_atl_query).collect()\n",
    "    if len(error_atl_df) > 0:\n",
    "        # Create a list to store the data\n",
    "        error_list_atl = []\n",
    "        for curr_row in error_atl_df:\n",
    "            error_list_atl.append(curr_row.asDict(True))\n",
    "        # Create the DataFrame with data and headers\n",
    "        atl_df = pd.DataFrame(error_list_atl)\n",
    "    else:\n",
    "        # Create an empty DataFrame with headers\n",
    "        atl_df = pd.DataFrame(columns=error_atl_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94aec806-c14f-4092-a9cd-b6a8dfb454e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Create the CSV file name using the current date\n",
    "now = datetime.datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n",
    "success_csv_file_name = f\"{s3_success_filename}{now}.csv\"\t\n",
    "error_csv_file_name = f\"{s3_error_filename}{now}.csv\"\n",
    "atl_error_file_name = f\"{ATL_error_filename}.csv\"\n",
    "\n",
    "# Initialize success_csv_data as an empty string\n",
    "success_csv_data = \"\"\n",
    "if usecase_id in usecase_set_1:\n",
    "    success_csv_data = success_df.to_csv(index=False)\n",
    "elif usecase_id in usecase_set_2:\n",
    "    # Write the header without quoting\n",
    "    header_csv_data = success_df.columns.to_list()\n",
    "    header_csv_str = ','.join(header_csv_data) + '\\n'  # Convert header to a CSV string\n",
    "    # Write the data with quoting\n",
    "    data_csv_data = success_df.to_csv(index=False, sep=',', quoting=csv.QUOTE_ALL, header=False)\n",
    "    # Combine the header and data\n",
    "    success_csv_data = header_csv_str + data_csv_data\n",
    "\n",
    "# Common error_csv_data line for all usecase\n",
    "error_csv_data = error_df.to_csv(index=False)   \n",
    "\n",
    "# Write the dynamic CSV file to the S3 bucket folder for success and error files\n",
    "s3_client.put_object(Bucket=s3_bucket, Key=s3_folder_path_arc + folder_name + '/' + success_csv_file_name, Body=success_csv_data)\n",
    "s3_client.put_object(Bucket=s3_bucket, Key=s3_folder_path_arc + folder_name + '/' + error_csv_file_name, Body=error_csv_data)\n",
    "\n",
    "# Remove the existing CSV files in the STG path\n",
    "response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=s3_folder_path_STG)\n",
    "for obj in response.get('Contents', []):\n",
    "    if obj['Key'].endswith('.csv'):\n",
    "        s3_client.delete_object(Bucket=s3_bucket, Key=obj['Key'])\n",
    "        print(f\"Deleted object: {obj['Key']}\")\n",
    "\n",
    "# Copy the new CSV file to the STG path\n",
    "s3_client.put_object(Bucket=s3_bucket, Key=s3_folder_path_STG + success_csv_file_name, Body=success_csv_data)\n",
    "\n",
    "if usecase_id == '514':\n",
    "    # ATL error data \n",
    "    atl_csv_data = atl_df.to_csv(index=False)\n",
    "\n",
    "# Write the dynamic CSV file to the ATL S3 bucket folder for success and error files\n",
    "if usecase_id == '514':\n",
    "    # DSOC ATL archile file load\n",
    "    s3_client.put_object(Bucket=ATL_s3_bucket, Key=ATL_s3_folder_path_arc + folder_name_ATL + '/' + atl_error_file_name, Body=atl_csv_data)\n",
    "    # remove file from DSOC ATL STG path\n",
    "    response = s3_client.list_objects_v2(Bucket=ATL_s3_bucket, Prefix=ATL_s3_folder_path_STG)\n",
    "    for obj in response.get('Contents', []):\n",
    "        if obj['Key'].endswith('.csv'):\n",
    "            s3_client.delete_object(Bucket=ATL_s3_bucket, Key=obj['Key'])\n",
    "            print(f\"Deleted object: {obj['Key']}\")\n",
    "    # DSOC ATL STG file load\n",
    "    s3_client.put_object(Bucket=ATL_s3_bucket, Key=ATL_s3_folder_path_STG + atl_error_file_name, Body= atl_csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09e50b73-463a-4a46-a88a-54131bba6d84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class s3Load:\n",
    "   def construct_microservice_response(self):\n",
    "    resp = {\n",
    "      \"nge_response\": {\n",
    "        \"status\": 200,\n",
    "        \"body\": \"s3 file load completed\"\n",
    "      }\n",
    "    }\n",
    "    self.response = resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94609b38-14e4-40f5-8601-be41a4bbd3bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metadata_obj = s3Load()\n",
    "metadata_obj.construct_microservice_response()\n",
    "dbutils.notebook.exit(\n",
    "  metadata_obj.response\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NGE-Box-NB-S3-File-Loader-Job",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
