{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8922eaf2-8243-42dd-8ee6-bf712c4c5dd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%python\n",
    "## Import libraries\n",
    "import pyspark as ps\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import *\n",
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4796c3d-7f08-4830-afe5-c1a548be705b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class LoadProcessedTable:\n",
    "  def __init__(self):\n",
    "    self.query = \"Query About to fill\"\n",
    "    self.sql_df = pd.DataFrame()\n",
    "    self.metadata_json = {\"Metadata\":[]}    \n",
    "  \n",
    "  def logger(message):\n",
    "    print(message)\n",
    "    \n",
    "  \n",
    "  def set_query_string(self):\n",
    "    query = \"WITH t(usecase_id, hist_retention) AS (SELECT USECASE_ID,hist_retention from com_us_alyt_ngebox.metadata_usecase),s(SUGGEST_EXTERNAL_ID_VOD__C,NGEBox_Record_ID) AS (select p.SUGGEST_EXTERNAL_ID_VOD__C, p.NGEBox_Record_ID from com_us_alyt_ngebox.ngebox_suggestion_requests_processed p left join com_us_lake.rep_suggestion_vod__c s on p.SUGGEST_EXTERNAL_ID_VOD__C = s.SUGGESTION_EXTERNAL_ID_VOD__C where (s.SUGGESTION_EXTERNAL_ID_VOD__C is NULL) or (s.SUGGESTION_EXTERNAL_ID_VOD__C is not null and s.DISMISSED_VOD__C = 0)) update com_us_alyt_ngebox.NGEBox_Suggestion_Requests_Processed set  Sent_To_Veeva = current_date where  (nvl(HCP_NPI_OLD,' ') <> nvl(HCP_NPI,' ') or nvl(HCP_OMNI_ID_OLD,' ') <> nvl(HCP_OMNI_ID,' ') or  nvl(OWNERID_OLD,' ') <> nvl(OWNERID,' ')) and CAST(created_date as DATE) >= date_add(current_date, -(select hist_retention from t where t.usecase_id = usecase_id ))  and Not_Sent_To_Veeva_Reason is NULL and SUGGEST_EXTERNAL_ID_VOD__C in (select SUGGEST_EXTERNAL_ID_VOD__C from s); WITH t(usecase_id, hist_retention) AS (SELECT USECASE_ID,hist_retention from com_us_alyt_ngebox.metadata_usecase),s(SUGGEST_EXTERNAL_ID_VOD__C,NGEBox_Record_ID) AS (select p.SUGGEST_EXTERNAL_ID_VOD__C, p.NGEBox_Record_ID from com_us_alyt_ngebox.ngebox_suggestion_requests_processed p left join com_us_lake.rep_suggestion_vod__c s on p.SUGGEST_EXTERNAL_ID_VOD__C = s.SUGGESTION_EXTERNAL_ID_VOD__C where (s.SUGGESTION_EXTERNAL_ID_VOD__C is NULL) or (s.SUGGESTION_EXTERNAL_ID_VOD__C is not null and s.DISMISSED_VOD__C = 0)) update com_us_alyt_ngebox.NGEBox_Suggestion_Requests_Processed set Processed = 1 where CAST(created_date as DATE) >= date_add(current_date, -(select hist_retention from t where t.usecase_id = usecase_id )) and SUGGEST_EXTERNAL_ID_VOD__C in (select SUGGEST_EXTERNAL_ID_VOD__C from s);\"\n",
    "    self.query = query\n",
    "  \n",
    "  def run_query_and_set_sqldf(self):\n",
    "    query_list = self.query.split(\";\")\n",
    "    self.sql_df = []\n",
    "    for curr_query in query_list:\n",
    "      if curr_query.strip() == \"\":\n",
    "        continue\n",
    "      sql_df = sqlContext.sql(curr_query).collect() # type(sql_df) -> dict\n",
    "      self.sql_df = self.sql_df + sql_df\n",
    "    \n",
    "  def format_sqldf_to_json(self):\n",
    "    sql_output_rowlist = []\n",
    "    for curr_row in self.sql_df: \n",
    "      sql_output_rowlist.append (curr_row.asDict(True))\n",
    "    sql_df_new = pd.DataFrame(sql_output_rowlist)\n",
    "    sql_jsonstr = sql_df_new.to_json(orient=\"records\") # type(result) -> str\n",
    "    sql_jsonparse = json.loads(sql_jsonstr) # type(parsed_json) -> list\n",
    "    nb_json = {\"data\":sql_jsonparse} # type(metadata_json) -> dict\n",
    "    self.nb_json = nb_json\n",
    "    \n",
    "  def construct_microservice_response(self):\n",
    "    resp = {\n",
    "      \"nge_response\": {\n",
    "        \"status\": 200,\n",
    "        \"body\": self.nb_json\n",
    "      }\n",
    "    }\n",
    "    self.response = resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8619eea7-8f9b-4e0d-a24f-0172eeb566ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'nge_response': {'status': 200, 'body': {'data': [{'num_affected_rows': 0}, {'num_affected_rows': 80}]}}}"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "{'nge_response': {'status': 200, 'body': {'data': [{'num_affected_rows': 0}, {'num_affected_rows': 80}]}}}",
       "metadata": {},
       "type": "exit"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_obj = LoadProcessedTable()\n",
    "nb_obj.set_query_string()\n",
    "nb_obj.run_query_and_set_sqldf()\n",
    "nb_obj.format_sqldf_to_json()\n",
    "nb_obj.construct_microservice_response()\n",
    "dbutils.notebook.exit(\n",
    "  nb_obj.response\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "NGE-Box-NB-Processed-Table-Update-Job",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
